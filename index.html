<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Fenglin Liu</title>
  
  <meta name="author" content="Fenglin Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="image/oxford.png">
</head>

  <body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="77%" valign="middle">
        <p align="center">
          <name>Fenglin Liu</name>
        </p>
		<p>Hey there, welcome! 👋<br/>
		</p>
		<p>	
			Fenglin Liu is a DPhil/PhD student at the University of Oxford, where he is very fortunately
			supervised by Professor <a href="https://eng.ox.ac.uk/people/david-clifton/">David Clifton</a>.
			He is a <a href="https://www.ox.ac.uk/clarendon">Clarendon Scholar</a> at Oxford and a member of <a href="https://en.wikipedia.org/wiki/Magdalen_College,_Oxford">Magdalen College</a>.  
			Previously, he has been fortunate to study at Peking University, advised by Prof. <a href="http://xusun.org/">Xu Sun</a> and  
	        	Prof. <a>Yuexian Zou</a>.
			<p></p> 
			His research, which is/was fully-funded by the Clarendon Scholarship, Magdalen Graduate Scholarship, 
			and China National Scholarship, includes <strong><em>Natural Language Processing (NLP)</em></strong>, 
			and its application to clinical, i.e., <strong><em>Clinical NLP</em></strong>.  
		</p>
	      <!--
		he has been fortunate to work with Prof. <a href="http://xusun.org/">Xu Sun</a>,  
	        Prof. <a href="https://scholar.google.com/citations?user=ES8dGKUAAAAJ">Yuexian Zou</a>,
	        and Dr. <a href="https://jklj077.github.io/">Xuancheng Ren</a> at Peking University,
	        with Dr. <a href="https://scholar.google.com/citations?user=lslB5jkAAAAJ">Xian Wu</a> and Dr. <a>Shen Ge</a> at Tencent Medical AI Lab (JARVIS Lab),
	        with Dr. <a href="https://scholar.google.com/citations?user=KPp_12IAAAAJ">Yang Yang</a> and Dr. <a>Jessie Liu</a> at Oxford-Suzhou Centre for Advanced Research,
	        and with Prof. <a href="https://homes.cs.washington.edu/~swang/">Sheng Wang</a> at University of Washington.
	      -->
	        He has published papers at leading journals and conferences, e.g., TPAMI, NeurIPS, CVPR, ACL, EMNLP, NAACL. 
	        He has served as a <em>Senior Program Committee (SPC)</em> member for IJCAI; 
		<em>Program Committee (PC) member/Conference Reviewer</em> for NeurIPS, ICLR, ICML, CVPR, ICCV, ECCV, ACL, EMNLP, NAACL, EACL, COLING, AACL, ARR, AAAI, IJCAI, MICCAI; 
		<em>Journal Reviewer</em> for TPAMI, TCSVT, TMM, TASLP;
		He was awarded as the <em>Distinguished/Outstanding Reviewer</em> of CVPR, AAAI, IJCAI.
        </p>

	    	      <!--
		
	      -->

        <p align=center>
          <a href="mailto:fenglin.liu@eng.ox.ac.uk">Email</a> &nbsp/&nbsp
<!--
	  <a href="image/CV_FenglinLiu.pdf">CV</a> &nbsp/&nbsp
-->
          <a href="#research">Research</a> &nbsp/&nbsp
          <a href="#pub">Publications</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=eTEITdwAAAAJ">Google Scholar</a> &nbsp&nbsp
        </p>

        </td>
        <td width="32%">
        <img src="image/fenglinliu-emnlp.jpg" width="180">
        </td>
      </tr>
      </table>



<table width="101%" align="center" border="0" cellspacing="0" cellpadding="20">
      <a name="research"><tr></a>
        <td width="100%" valign="middle">
          <heading>🎓 Research Interests</heading>
          <p>

		  &bull; Currently, I aim at building multimodal interactive AI systems that can not only ground and reason over the external world signals, e.g., vision, audio, and knowledge, 
		  to understand human language, but also assist humans in decision-making and efficiently solve social concerns, i.e., clinical. 
		  <p></p>
		  <p>
		  &bull; As steps towards the goal, my research focuses on:<br/>
		  - <strong><em>Natural Language Processing</em></strong>: 
			  &nbsp;&nbsp;<smalll> Vision-and-Language, Language Generation, and Language Understanding.</smalll><br/>
		  - <strong><em>Machine Learning</em></strong> for NLP: 
			  &nbsp;&nbsp;<smalll>Representation, Few-shot, Transfer, Meta, Unsupervised Learning, and Pre-training.</smalll><br/>
		  - <strong><em>Clinical NLP</em></strong>: 
			  &nbsp;&nbsp;<smalll> Clinical Note, Radiology Report, Gene Ontology, and Time-series Data.</smalll><br/>

	  <p> 
<!--		
    		&bull; In the future, I would like to focus on the broader application of <strong><em>AI in Healthcare</em></strong>, 
		  enabling the multimodal interactive AI systems to advance the public healthcare, 
		  e.g., assist clinicians in their decision making with the prognostication, treatment planning, and monitoring of patients.
<!--		  
		  Therefore, I would like to focus on the broader application of <strong><em>AI in Healthcare</em></strong>: <br/>
		  - 2) <strong><em>Explainable/Interpretable AI</em></strong>, e.g., providing explanations for the decisions made by the systems and 
		  interpreting neural representations of language and vision, to better understand the multimodal interactive AI systems. 
-->
          </p>
        </td>
      </tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>🔥 News</heading>
          <p> 	  
		  <li> <strongsmall>[2022/12]</strongsmall> &nbsp;&nbsp;<smalll>
			  1 Journal paper is published at <strong>IEEE TPAMI 2022</strong> (Volume: 44, Issue: 12).</smalll><br/>
		  <li> <strongsmall>[2022/11]</strongsmall> &nbsp;&nbsp;<smalll>
			  The code and dataset for our NeurIPS2022 paper [<a href="https://arxiv.org/abs/2210.12777">arXiv</a>] are now available at [<a href="https://github.com/AI-in-Hospitals/Patient-Instructions">Github</a>]. 
		  	  Welcome to Fork and Star!</smalll><br/>
		  <li> <strongsmall>[2022/10]</strongsmall> &nbsp;&nbsp;<smalll>
			  Very honored to join <a href="https://eng.ox.ac.uk/chi/">CHI Lab</a> and be supervised by Prof. <a href="https://eng.ox.ac.uk/people/david-clifton/">David Clifton</a>. 
			  Sincerely thank you very much!</smalll><br/>
		  <li> <strongsmall>[2022/09]</strongsmall> &nbsp;&nbsp;<smalll>
			  3 Conference papers are accepted by <strong>NeurIPS 2022</strong>.</smalll><br/>
		  <li> <strongsmall>[2022/07]</strongsmall> &nbsp;&nbsp;<smalll>
			  Very glad to be recognized as a <font color="red"><strong>Distinguished Reviewer</strong></font> for <strong>IJCAI 2022</strong> [<a href="https://ijcai-22.org/distinguished-pc-members/">url</a>].</smalll><br/>
		  <li> <strongsmall>[2022/05]</strongsmall> &nbsp;&nbsp;<smalll>
			  1 Conference paper is accepted by <strong>KDD 2022</strong>.</smalll><br/>
		  <li> <strongsmall>[2022/04]</strongsmall> &nbsp;&nbsp;<smalll>
			  1 Conference paper is accepted by <strong>NAACL 2022 (Findings)</strong>.</smalll><br/>
		  <li> <strongsmall>[2022/01]</strongsmall> &nbsp;&nbsp;<smalll>
			  1 Journal paper is accepted by <strong>IEEE TPAMI 2022</strong>.</smalll><br/>
		  <li> <strongsmall>[2021/11]</strongsmall> &nbsp;&nbsp;<smalll>
			  Very glad to serve as a <strong>Mentor</strong> for <strong>ACL Year-Round Mentorship</strong> [<a href="https://mentorship.aclweb.org/">url</a>].</smalll><br/>
		  <li> <strongsmall>[2021/09]</strongsmall> &nbsp;&nbsp;<smalll>
			  1 Conference paper is accepted by <strong>NeurIPS 2021</strong>.</smalll><br/>
		  <li> <strongsmall>[2021/07]</strongsmall> &nbsp;&nbsp;<smalll>
			  Very glad to serve as a novel <font color="red"><strong>Program Committee Board (PCB)</strong></font> member for <strong>IJCAI</strong>, 2022-2024.</smalll><br/>
		  <li> <strongsmall>[2021/06]</strongsmall> &nbsp;&nbsp;<smalll>
			  1 Conference paper is accepted by <strong>MICCAI 2021</strong>.</smalll><br/>
		  <li> <strongsmall>[2021/05]</strongsmall> &nbsp;&nbsp;<smalll>
			  3 Conference papers are accepted by <strong>ACL 2021</strong>.</smalll><br/>
		  <li> <strongsmall>[2021/05]</strongsmall> &nbsp;&nbsp;<smalll>
			  Very glad to be recognized as an <font color="red"><strong>Outstanding Reviewer</strong></font> for <strong>CVPR 2021</strong> [<a href="http://cvpr2021.thecvf.com/node/184">url</a>].</smalll><br/>
		  <li> <strongsmall>[2021/05]</strongsmall> &nbsp;&nbsp;<smalll>
			  Very glad to be recognized as a <font color="red"><strong>Top Reviewer</strong></font> for <strong>AAAI 2021</strong> [<a href="https://aaai.org/Conferences/AAAI-21/wp-content/uploads/2021/05/AAAI-21-Program-Committee.pdf">url</a>].</smalll><br/>
		  <li> <strongsmall>[2021/03]</strongsmall> &nbsp;&nbsp;<smalll>
			  1 Conference paper is accepted by <strong>CVPR 2021</strong>.</smalll><br/>
		  <li> <strongsmall>[2021/01]</strongsmall> &nbsp;&nbsp;<smalll>
			  Very glad to serve as a <font color="red"><strong>Senior Program Committee (SPC)</strong></font> member for <strong>IJCAI 2021</strong> [<a href="https://ijcai-21.org/senior-program-committee-members/">url</a>].</smalll><br/>	  
		  <li> <strongsmall>[2020/11]</strongsmall> &nbsp;&nbsp;<smalll>
			  2 Conference papers are accepted by <strong>AAAI 2021</strong>.</smalll><br/>
		  <li> <strongsmall>[2020/10]</strongsmall> &nbsp;&nbsp;<smalll>
			  2 Conference papers are accepted by <strong>COLING 2020</strong>.</smalll><br/>				  
		  <li> <strongsmall>[2020/09]</strongsmall> &nbsp;&nbsp;<smalll>
			  1 Conference paper is accepted by <strong>NeurIPS 2020</strong>.</smalll><br/>		  		  
		  <li> <strongsmall>[2019/11]</strongsmall> &nbsp;&nbsp;<smalll>
			  1 Conference paper is accepted by <strong>AAAI 2020</strong>.</smalll><br/>
		  <li> <strongsmall>[2019/09]</strongsmall> &nbsp;&nbsp;<smalll>
			  1 Conference paper is accepted by <strong>NeurIPS 2019</strong>.</smalll><br/>
    		  <li> <strongsmall>[2019/04]</strongsmall> &nbsp;&nbsp;<smalll>
			  1 Conference paper is accepted by <strong>IJCAI 2019</strong>.</smalll><br/>
		  <li> <strongsmall>[2018/08]</strongsmall> &nbsp;&nbsp;<smalll>
			  1 Conference paper is accepted by <strong>EMNLP 2018</strong>.</smalll><br/>
		  <li> <strongsmall>[2018/01]</strongsmall> &nbsp;&nbsp;<smalll>
				  Very honored to join <a href="https://lancopku.github.io/">Lanco Lab</a> and be supervised by Prof. <a href="http://xusun.org/">Xu Sun</a> and Dr. <a href="https://jklj077.github.io/">Xuancheng Ren</a>. 
				  Sincerely thank them very much!</smalll><br/>
          </p>
        </td>
      </tr>
</table>

	
<!--
<p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <heading>&nbsp; &nbsp; &nbsp;Education</heading>

        <tr>
          <td width="10%">
            <img src='image/oxford.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>[2022/10 - Present] <a href="https://www.ox.ac.uk/">University of Oxford</a>, UK</stronghuge><br />
          &bull; PhD in Engineering Science; <font color="red"><em>Clarendon Scholar</em></font><br/> 
          &bull; Working on Machine Learning and AI in Healthcare <br />
          &bull; Supervisor: &nbsp; Prof. <a href="https://eng.ox.ac.uk/people/david-clifton/">David Clifton</a><br/>

	  
          <br />
          </p>
        </td>
      </tr>

        <tr>
          <td width="10%">
            <img src='image/pku.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>[2019/09 - 2022/07] <a href="http://www.pku.edu.cn/">Peking University</a>, China</stronghuge><br />
          &bull; Master in Computer Science; <font color="red"><em>Outstanding Graduates </em></font><br/> 
	  &bull; Major courses in Computer Science and Machine Learning<br />
          &bull; Supervisors: &nbsp; Prof. <a href="http://xusun.org/">Xu Sun</a> and Prof. <a href="https://scholar.google.com/citations?user=ES8dGKUAAAAJ">Yuexian Zou</a><br/>

		  &bull; GPA Ranking: <font color="red"><strong>1/103 (1.0%)</strong></font> <br />

          <br />
          </p>
        </td>
      </tr>
	      
	    <tr>
          <td width="10%">
            <img src='image/bupt.jpg' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>[2015/09 - 2019/06] <a href="https://www.bupt.edu.cn/">Beijing University of Posts and Telecommunications</a>, China</stronghuge><br />
          &bull; Bachelor of Telecommunication Engineering; <font color="red"><em>Outstanding Graduates </em></font><br/>
		  &bull; Major courses in Telecommunication Engineering and Computer Science<br />
		  &bull; Supervisors: &nbsp; Prof. <a href="http://xusun.org/">Xu Sun</a> and Dr. <a href="https://jklj077.github.io/">Xuancheng Ren</a>.<br />
		  

                  &bull; GPA Ranking: <font color="red"><strong>6/597 (1.0%)</strong></font><br />

          </p>
        </td>
      </tr>
	  
      </table>

-->
	  


<p></p> 
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <heading>&nbsp; &nbsp; &nbsp;👑 Research Experiences</heading>

	    <tr>
          <td width="10%">
            <img src='image/sjtu.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge> <a href="https://www.shsmu.edu.cn/gwyw/">School of Public Health</a>,
		  <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a>, Shanghai, China.
		  </stronghuge><br />
          <huge>[2021/12 - Present]&nbsp;&nbsp; <em>Research Intern</em></huge><br />
          Supervisor: &nbsp; Prof. <a href="https://scholar.google.com/citations?user=KPp_12IAAAAJ">Yang Yang</a>.<br/>
          <li> Working on Machine Learning and Clinical NLP. <br/>
		  <li> Published a paper on NeurIPS. Submitting a paper to Nature Communications.
          </p>
        </td>
      </tr> 
	      
	    <tr>
          <td width="10%">
            <img src='image/oxford.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge> <a href="https://oscar.web.ox.ac.uk/">Oxford-Suzhou Centre for Advanced Research</a>,
		  <a href="https://www.ox.ac.uk/">University of Oxford</a>, Oxford, UK.
		  </stronghuge><br />
          <huge>[2021/09 - 2022/09]&nbsp;&nbsp; <em>Research Intern</em></huge><br />
          Supervisor: &nbsp; Prof. <a href="https://eng.ox.ac.uk/people/david-clifton/">David Clifton</a>.<br/>
          <li> Working on Machine Learning and Clinical NLP. <br/>
		  <li> Published two papers on NeurIPS.
          </p>
        </td>
      </tr> 

        <tr>
          <td width="10%">
            <img src='image/uw.png' width="100">
            </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge><a href="https://www.cs.washington.edu/">Paul G. Allen School of Computer Science & Engineering</a>, 
 		<a href="https://www.washington.edu/">University of Washington</a>, Seattle, USA.
		  </stronghuge><br />
          <huge>[2021/04 - 2022/05]&nbsp;&nbsp; <em>Research Intern</em></huge><br />
          Supervisor: &nbsp; Prof. <a href="https://homes.cs.washington.edu/~swang/">Sheng Wang</a>.<br/>
          <li> Working on Clinical NLP (Gene Ontology). <br/>
		  <li> Published papers on NeurIPS and KDD.
          </p>
        </td>
      </tr>  


<!--
	    <tr>
          <td width="10%">
            <img src='image/hch.jpg' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge> Harbin Chest Hospital, Harbin, China.
		  </stronghuge><br />
          <huge>[2020/07 - 2020/08]&nbsp;&nbsp; <em>Summer Intern</em></huge><br />
          Advisors: &nbsp; Chief Physician: <a>Xiaoxia Xie</a>, and Physician-in-Charge: <a>Jing Zhang</a>.<br/>
          <li> Learning on Radiology/Chest X-ray Report Processing. <br/>
		  <li> Published papers on NeurIPS, ACL, CVPR and MICCAI.
          </p>
        </td>
      </tr>  
-->

	    <tr>
          <td width="10%">
            <img src='image/tencent.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge><a href="https://jarvislab.tencent.com/index-en.html">Medical AI Lab (JARVIS Lab)</a>, 
		  <a href="https://www.tencent.com/">Tencent</a>, Beijing, China.
		  </stronghuge><br />
          <huge>[2019/01 - 2022/09]&nbsp;&nbsp; <em>Research Intern</em></huge><br />
          Mentors: &nbsp; Principal Researcher: <a href="https://scholar.google.com/citations?user=lslB5jkAAAAJ">Xian Wu</a> and <a>Shen Ge</a>.<br/>
          <li> Working on Machine Learning and Clinical NLP. <br/>
		  <li> Published papers on NeurIPS, TPAMI, ACL, CVPR, KDD, AAAI, MICCAI.
          </p>
        </td>
      </tr>  


	    <tr>
          <td width="10%">
            <img src='image/pku.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge><a href="https://lancopku.github.io/">Language Computing and Machine Learning Group (LANCO)</a>, 
		  <a href="http://www.pku.edu.cn/">Peking University</a>, Beijing, China.
		  </stronghuge><br />
          <huge>[2018/01 - 2022/09]&nbsp;&nbsp; <em>Research Assistant</em></huge><br />
          Supervisors: &nbsp; Prof. <a href="http://xusun.org/">Xu Sun</a> and Dr. <a href="https://jklj077.github.io/">Xuancheng Ren</a>.<br/>
          <li> Working on Machine Learning and Natural Language Processing. <br/>
		  <li> Published papers on NeurIPS, TPAMI, ACL, EMNLP, IJCAI, COLING.
          </p>
        </td>
      </tr>
       </table>
	
	
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <a name="pub"> </a><heading>&nbsp; &nbsp; &nbsp;📝 Publications</heading>
	      <strongsmall>
			<a href="https://scholar.google.com/citations?user=eTEITdwAAAAJ"><strongsmall>[Google Scholar]</strongsmall></a>
		</strongsmall>
	      
		
<!--		
		SORT BY 
		<select name="pageselect" onchange="self.location.href=options[selectedIndex].value">
		<option value="index.html#pub">Area</option>
		<option value="date.html#pub">Date</a></option>
		</select>
-->
 
<!--
      <table width="100%" align="center" border="1" cellspacing="0" cellpadding="20">
 
          <strong><big>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&bull; 2022</big></strong>
	 
      </table>
-->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td width="15%">
        <a href="paper/NeurIPS2022_PI/introduction.png"><img src='paper/NeurIPS2022_PI/introduction.png'  width="200" height="110"></a>
        </td>
      <td valign="top" width="75%">
	 <a name="[10]"><big>[10]</big></a> <em>Retrieve, Reason, and Refine: Generating Accurate and Faithful Patient Instructions</em><br>
     <strong>Fenglin Liu</strong>, Bang Yang, Chenyu You, Xian Wu, Shen Ge, Zhangdaihong Liu, Xu Sun, Yang Yang, David A. Clifton<br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2022</strong></font>,
	      <a href="https://openreview.net/forum?id=dp0zWsdOV1h">[paper link]</a>, 
	      <a href="paper/NeurIPS2022_PI/NeurIPS2022_PI.pdf">[pdf]</a>,
	      <a href="">[ppt]</a>,
	      <a href="paper/NeurIPS2022_PI/NeurIPS2022_Appendix.pdf">[appendix]</a>,
	      <a href="paper/NeurIPS2022_PI/nips2022_poster.pdf">[poster]</a>,
	      <a href="https://github.com/AI-in-Hospitals/Patient-Instructions">[code]</a><br>
	      
        <p></p>
		<p>&bull; We propose a new task of Patient Instruction (PI) generation which attempts to generate accurate and faithful PIs,
			which guide the patients how to manage their conditions after discharge based on their health records during hospitalization.
		To address this task, we build a dataset PI and present an effective approach Re<sup>3</sup>Writer: Retrieve, Reason, and Refine.</p>
	</td>
    </tr>
   </table>

	      
 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td width="15%">
        <a href="paper/NeurIPS2022_CATformer/introduction.png"><img src='paper/NeurIPS2022_CATformer/introduction.png'  width="200" height="110"></a>
        </td>
      <td valign="top" width="75%">
	 <a name="[9]"><big>[9]</big></a> <em>Class-Aware Generative Adversarial Transformers for Medical Image Segmentation</em><br>
     Chenyu You, Ruihan Zhao, <strong>Fenglin Liu</strong>, Sandeep Chinchali, Ufuk Topcu, Lawrence Staib, James S Duncan<br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2022</strong></font>,
	      <a href="https://openreview.net/forum?id=aqLugNVQqRw">[paper link]</a>, 
	      <a href="paper/NeurIPS2022_CATformer/NeurIPS2022_CATformer.pdf">[pdf]</a>, 
	      <a href="">[poster]</a>,
	      <a href="">[code]</a><br>
	      
        <p></p>
		<p>&bull; We make the first attempt to build a GAN using a transformer-based architecture for the 2D medical image segmentation task.
			We incorporate the pyramid structure into the generator to learn rich global and local multi-scale spatial representations, 
			and also devise a novel class-aware transformer module by progressively learning the interesting regions correlated with the semantic structures of images.</p>
	</td>
    </tr>
   </table>
	     
	      
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td width="15%">
        <a href="paper/NeurIPS2022_EMCL/introduction.png"><img src='paper/NeurIPS2022_EMCL/introduction.png'  width="200" height="110"></a>
        </td>
      <td valign="top" width="75%">
	 <a name="[8]"><big>[8]</big></a> <em>Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations</em><br>
     Peng Jin, JinFa Huang, <strong>Fenglin Liu</strong>, Xian Wu, Shen Ge, Guoli Song, David A. Clifton, Jie Chen<br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2022</font> (<font color="red">Spotlight</font>)</strong>,
	      <a href="https://openreview.net/forum?id=ijzm0EhAY_w">[paper link]</a>, 
	      <a href="paper/NeurIPS2022_EMCL/NeurIPS2022_EMCL.pdf">[pdf]</a>, 
	      <a href="paper/NeurIPS2022_EMCL/nips2022_poster_emcl.png">[poster]</a>,
	      <a href="https://github.com/jpthu17/EMCL">[code]</a><br>
	      
        <p></p>
		<p>&bull; To alleviate the cross-modal representation bias, we reformulate the contrastive learning for video-and-language representations
			into an expectation-maximization iteration manner and propose a plug-and-play feature projection module named Expectation-Maximization Contrastive Learning (EMCL),
			which learns the subspace that aims to become semantic-relevant and compact representation.</p>
	</td>
    </tr>
   </table>




    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">

     <a href="paper/TPAMI2022_UVC-VI/framework.png"><img src='paper/TPAMI2022_UVC-VI/framework.png' width="200" height="110"></a>

      </td>
      <td valign="top" width="75%">
	 <a name="[7]"><big>[7]</big></a> <em>Aligning Source Visual and Target Language Domains for Unpaired Video Captioning</em><br>
     <strong>Fenglin Liu</strong>, Xian Wu, Chenyu You, Shen Ge, Yuexian Zou, Xu Sun<br>
        IEEE Transactions on Pattern Analysis and Machine Intelligence <strong>(<font color="#a82e2e">TPAMI 2022</font>, <font color="red">IF: 24.314</font>)</strong>,
                <a href="https://ieeexplore.ieee.org/document/9633156">[paper link]</a>, 
	        <a href="paper/TPAMI2022_UVC-VI/TPAMI2022__Unpaired_Video_Captioning.pdf">[pdf]</a><br>
	      
        <p></p>
		<p>&bull; <em>We make the first attempt to conduct unpaired video captioning under various low-resource language application scenarios, 
			e.g., French, German and Chinese, in which the video-caption pairs are not available</em>. <br>
		&bull; We present the Unpaired Video Captioning with Visual Injection system, which even exceeds several recently proposed supervised systems.</p>
	</td> 
    </tr>
   </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td width="15%">
        <a href="paper/NeurIPS2021_KGAE/framework.png"><img src='paper/NeurIPS2021_KGAE/framework.png'  width="200" height="110"></a>
        </td>
      <td valign="top" width="75%">
	 <a name="[6]"><big>[6]</big></a> <em>Auto-Encoding Knowledge Graph for Unsupervised Medical Report Generation</em><br>
     <strong>Fenglin Liu</strong>, Chenyu You, Xian Wu, Shen Ge, Sheng Wang, Xu Sun<br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2021</strong></font>,
	      <a href="https://proceedings.neurips.cc/paper/2021/hash/876e1c59023b1a0e95808168e1a8ff89-Abstract.html">[paper link]</a>, 
	      <a href="paper/NeurIPS2021_KGAE/NeurIPS2021_KGAE.pdf">[pdf]</a>, 
	      <a href="paper/NeurIPS2021_KGAE/nips21_poster.pdf">[poster]</a><br>
	      
        <p></p>
		<p>&bull; We make the first attempt to train a medical report generation model in an unsupervised manner. <br>
		&bull; We propose the Knowledge Graph Auto-Encoder (KGAE). <em>Without any image-report pairs</em>, 
			KGAE can extract the knowledge representations of both image and report from the knowledge graph to bridge the visual and textual domains, 
			and generate desirable reports by being trained in the auto-encoding pipeline. </p>
	</td>
    </tr>
   </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">

     <a href="paper/CVPR2021_PPKED/framework.png"><img src='paper/CVPR2021_PPKED/framework.png' width="200" height="110"></a>

      </td>
      <td valign="top" width="75%">
	 <a name="[5]"><big>[5]</big></a> <em>Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation</em><br>
     <strong>Fenglin Liu</strong>, Xian Wu, Shen Ge, Wei Fan, Yuexian Zou<br>
        In Proceedings of <font color="#a82e2e"><strong>CVPR 2021</strong></font>,
	      <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Exploring_and_Distilling_Posterior_and_Prior_Knowledge_for_Radiology_Report_CVPR_2021_paper.pdf">[paper link]</a>, 
	      <a href="paper/CVPR2021_PPKED/CVPR2021_PPKED.pdf">[pdf]</a>, 
	      <a href="paper/CVPR2021_PPKED/CVPR2021_PPKED_ppt.pdf">[ppt]</a>, 
	      <a href="paper/CVPR2021_PPKED/CVPR2021_PPKED_poster.pdf">[poster]</a><br>
	      
        <p></p>
		<p>&bull; We propose to explore and distill posterior and prior knowledge by first examining the abnormal regions, and then relying on the prior medical knowledge and prior working experience to write accurate radiology reports. <br>
		&bull;  Media Coverage (<strong><font color="red">17,000 reads</font></strong>, in Chinese): 
			<img class="img-thumbnail" src="image/liangziwei.jpg" alt="" height="16"><a href="https://mp.weixin.qq.com/s/zGBpTyycIMCkXJBrEdYdfA"> QbitAI (量子位)</a>; 
			<img class="img-thumbnail" src="image/jiqizhixin.jpg" alt="" style="" height="16"><a href="https://mp.weixin.qq.com/s/4Tp5ReW0tHozWX1hKexzEQ"> Synced (机器之心)</a>.</p>
      </td>
    </tr>
   </table>
    

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
      <a href="paper/ACL2021_CMCL/framework.png"><img src='paper/ACL2021_CMCL/framework.png'  width="200" height="110"></a>
      </td>
      <td valign="top" width="75%">
	 <a name="[4]"><big>[4]</big></a> <em>Competence-based Multimodal Curriculum Learning for Medical Report Generation</em><br>
     <strong>Fenglin Liu</strong>, Shen Ge, Xian Wu<br>
        In Proceedings of <font color="#a82e2e"><strong>ACL 2021</font> (<font color="red">Oral</font>)</strong>,
              <a href="https://aclanthology.org/2021.acl-long.234/">[paper link]</a>, 
	      <a href="paper/ACL2021_CMCL/ACL2021_CMCL.pdf">[pdf]</a>, 
	      <a href="paper/ACL2021_CMCL/ACL2021_CMCL_ppt.pdf">[ppt]</a>, 
	      <a href="https://drive.google.com/file/d/1ngMdclHQ-V-_g87xW8LZk8wlcY_3qxBo/view?usp=sharing">[video]</a><br>
	      
        <p></p>
		<p>&bull; We introduce the competence-based multimodal curriculum learning in medical report generation, 
			which enables the models to gradually proceed from easy samples to more complex ones in training, 
			alleviating the data bias and thus improving the training efficiency.</p>
      </td>
    </tr>
   </table>

<!--
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
       <a href="paper/ACL2021_CA/contrastive.png"><img src='paper/ACL2021_CA/contrastive.png'  width="200" height="110"></a>
      </td>
      <td valign="top" width="75%">
	 <a name="[8]"><big>[8]</big></a> <em>Contrastive Attention for Automatic Chest X-ray Report Generation</em><br>
     <strong>Fenglin Liu</strong>, Changchang Yin, Xian Wu, Shen Ge, Ping Zhang, Xu Sun<br>
        In Findings of <strong><font color="#a82e2e">ACL 2021</font></strong>
	      <a href="https://aclanthology.org/2021.findings-acl.23/">[paper link]</a>, 
	      <a href="paper/ACL2021_CA/ACL2021_CA.pdf">[pdf]</a>, <a href="paper/ACL2021_CA/ACL2021_CA_ppt.pdf">[ppt]</a>, 
	      <a href="paper/ACL2021_CA/ACL2021_CA_poster.pdf">[poster]</a>, 
	      <a href="https://drive.google.com/file/d/1QKLAHdUhyqeoigqYRFB3s5qmbW6c0Nko/view?usp=sharing">[video]</a><br>

        <p></p>
		<p>&bull; We propose the Contrastive Attention model to capture abnormal regions by contrasting the input image and normal images to distill the contrastive information, which can better represent the visual features of abnormal regions and thus improve the performance of Chest X-ray report generation models. </p>
      </td>
    </tr>
   </table>
-->

<!--
      <table width="100%" align="center" border="1" cellspacing="0" cellpadding="20">
 
          <strong><big>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&bull; Area: &nbsp; Machine Learning</big></strong> [2019 – Present]
 
      </table>
-->



    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
       <a href="paper/NeurIPS2020_Prophet/framework.png"><img src='paper/NeurIPS2020_Prophet/framework.png'  width="200" height="110"></a>
      </td>
      <td valign="top" width="75%">
	<a name="[3]"><big>[3]</big></a> <em>Prophet Attention: Predicting Attention with Future Attention</em><br>
       <strong>Fenglin Liu</strong>, Xuancheng Ren, Xian Wu, Shen Ge, Wei Fan, Yuexian Zou, Xu Sun <br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2020</strong></font>,
	      <a href="https://proceedings.neurips.cc/paper/2020/file/13fe9d84310e77f13a6d184dbf1232f3-Paper.pdf">[paper link]</a>,
	      <a href="paper/NeurIPS2020_Prophet/NeurIPS2020_Prophet.pdf">[pdf]</a>,
	      <a href="paper/NeurIPS2020_Prophet/NeurIPS2020_Prophet_ppt.pdf">[ppt]</a>,
	      <a href="paper/NeurIPS2020_Prophet/NeurIPS2020_Poster.pdf">[poster]</a>
	      <br>
        <p></p>
		<p> &bull; We propose the Prophet Attention to calculate attentional weights based on future information,
			and force the attention model to learn to correctly ground each generated word to proper image regions.<br/>
			&bull; <font color="red"><strong>Leaderboard #1</strong></font> on Microsoft COCO Image Captioning Challenge from June 2, 2020 to September 3, 2020. </p>
      </td>
    </tr>
   </table>

<!--
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
       <a href="paper/AAAI2020_aimNet/framework.png"><img src='paper/AAAI2020_aimNet/framework.png'  width="200" height="110"></a>
      </td>
      <td valign="top" width="75%">
	 <a name="[4]"><big>[4]</big></a> <em>Federated Learning for Vision-and-Language Grounding Problems</em><br>
       <strong>Fenglin Liu</strong>, Xian Wu, Shen Ge, Wei Fan, Yuexian Zou <br>
        In Proceedings of <font color="#a82e2e"><strong>AAAI 2020</font> (<font color="red">Oral</font>)</strong>,
	      <a href="https://ojs.aaai.org//index.php/AAAI/article/view/6824">[paper link]</a>,
	      <a href="paper/AAAI2020_aimNet/AAAI2020_aimNet.pdf">[pdf]</a>,
	      <a href="paper/AAAI2020_aimNet/AAAI2020_aimNet_ppt.pdf">[ppt]</a>
	      <br>
        <p></p>
	      <p> &bull; We propose a federated learning framework to obtain various types of image representations from different vision-and-language tasks 
	      without the sharing of downstream task data, which are then fused together to form fine-grained image representations.
	      The representations merge useful features from different tasks, 
	      and are thus much more powerful than the original representations alone in individual tasks. </p>
      </td>
    </tr>
   </table>
-->

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
       <a href="paper/NeurIPS2019_MIA/visualization.png"><img src='paper/NeurIPS2019_MIA/visualization.png' width="200" height="110"></a>
      </td>
      <td valign="top" width="75%">
	 <a name="[2]"><big>[2]</big></a> <em>Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations</em><br>
       <strong>Fenglin Liu</strong>, Yuanxin Liu, Xuancheng Ren, Xiaodong He, Xu Sun <br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2019</strong></font>,
	      <a href="https://proceedings.neurips.cc/paper/2019/file/9fe77ac7060e716f2d42631d156825c0-Paper.pdf">[paper link]</a>,
	      <a href="paper/NeurIPS2019_MIA/NeurIPS2019_MIA.pdf">[pdf]</a>,
	      <a href="paper/NeurIPS2019_MIA/NeurIPS2019_MIA_Poster.pdf">[poster]</a>,
	      <a href="https://github.com/fenglinliu98/MIA">[code]</a>
	      <br>
        <p></p>
		<p> &bull; We aim at representing an image with a set of integrated visual regions and corresponding textual concepts, 
			reflecting certain semantics. To this end, we build the Mutual Iterative Attention (MIA) module, 
			which integrates correlated visual features and textual concepts, respectively, by aligning the two modalities. </p>
      </td>
    </tr>
   </table>

<!--
      <table width="100%" align="center" border="1" cellspacing="0" cellpadding="20">
    
          <strong><big>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &bull; Area: &nbsp; Multimodal NLP  </big></strong> [2017 – Present]
 
      </table>
-->

<!--
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
       <a href="paper/ACL2021_O2NA/introduction.png"><img src='paper/ACL2021_O2NA/introduction.png'  width="200" height="110"></a>
      </td>
      <td valign="top" width="75%">
	 <a name="[4]"><big>[4]</big></a> <em>O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning</em><br>
	      <strong>Fenglin Liu</strong>, Xuancheng Ren, Xian Wu, Bang Yang, Shen Ge, Xu Sun <br>
        In Findings of <strong><font color="#a82e2e">ACL 2021</font></strong>,
	      <a href="https://aclanthology.org/2021.findings-acl.24/">[paper link]</a>,
	      <a href="https://aclanthology.org/2021.findings-acl.24.pdf">[pdf]</a>,
	      <a href="https://github.com/yangbang18/Non-Autoregressive-Video-Captioning">[code]</a>
	      <br>
        <p></p> 
	      <p> &bull; We introduce the problem of controllable video captioning in the sense of controlled contents, 
		      which has more practical values than the existing studies on syntactic variations.  
		      We propose the O2NA, which is based on the non-autoregressive decoding method and encouraged to describe the focused objects user cares about. <br/>
		  &bull; <font color="red"><strong>3x faster inference speed</strong></font> than previous works. </p>
	      </p> 
      </td>
    </tr>
   </table>
-->




<!--
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
       <a href="paper/IJCAI2019_GLIED/visualization.png"><img src='paper/IJCAI2019_GLIED/visualization.png'  width="200" height="110"></a>
      </td>
      <td valign="top" width="75%">
	 <a name="[2]"><big>[2]</big></a> <em>Exploring and Distilling Cross-Modal Information for Image Captioning</em><br>
       <strong>Fenglin Liu</strong>, Xuancheng Ren, Yuanxin Liu, Kai Lei, Xu Sun <br>
        In Proceedings of <font color="#a82e2e"><strong>IJCAI 2019</font> (<font color="red">Oral</font>)</strong>,
	      <a href="https://www.ijcai.org/proceedings/2019/708">[paper link]</a>,
	      <a href="paper/IJCAI2019_GLIED/IJCAI2019_GLIED.pdf">[pdf]</a>,
	      <a href="paper/IJCAI2019_GLIED/IJCAI2019_GLIED_ppt.pdf">[ppt]</a>
	      <br>
        <p></p>
		<p> &bull;  We propose an approach to <em>globally</em> "captures the inherent spatial and relational groupings of the individual image regions and attribute words 
			for an aspect-based image representation", and <em>locally</em> "extracts fine-grained source information for precise and accurate word selection". <br/>
			&bull; <font color="red"><strong>State-of-the-art</strong></font> on Image Captioning 
			with <font color="red"><strong>fewer parameters</strong></font> and <font color="red"><strong>faster computation</strong></font>. </p>
      </td>
    </tr>
   </table>
-->


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
      <a href="paper/EMNLP2018_simNet/framework.png"><img src='paper/EMNLP2018_simNet/framework.png'  width="200" height="110"></a>
      </td>
      <td valign="top" width="75%">
	 <a name="[1]"><big>[1]</big></a> <em>simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions</em><br>
        <strong>Fenglin Liu</strong>, Xuancheng Ren, Yuanxin Liu, Houfeng Wang, Xu Sun <br>
        In Proceedings of <font color="#a82e2e"><strong>EMNLP 2018</font> (<font color="red">Oral</font>)</strong>,
	      <a href="http://aclweb.org/anthology/D18-1013">[paper link]</a>,
	      <a href="paper/EMNLP2018_simNet/EMNLP2018_simNet.pdf">[pdf]</a>,
	      <a href="paper/EMNLP2018_simNet/EMNLP2018_simNet_ppt.pdf">[ppt]</a>,
	      <a href="https://vimeo.com/305210529">[video]</a>,
	      <a href="https://github.com/lancopku/simNet">[code]</a>
	      <br>
        <p></p>
		<p> &bull;  We propose the simNet, including an importance-based merging gate 
			to effectively merge and balance the information in the image and the topics, 
			to generate detailed and comprehensive image captions. <br/>
			&bull; <font color="red"><strong>State-of-the-art</strong></font> on Image Captioning. </p>
      </td>
    </tr>
   </table>

 
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>👨‍💻 Services</heading>
          <p>
		  <li> <strong>[Senior Program Committee (SPC) Member]</strong>:<br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://ijcai-21.org/senior-program-committee-members/">
			  International Joint Conference on Artificial Intelligence (IJCAI)</a> 
			  <br/><br/>
		  <li> <strong>[Program Committee Member/Conference Reviewer]</strong>:<br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://nips.cc/">
			  Annual Conference on Neural Information Processing Systems (NeurIPS)</a> 
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://iclr.cc/">
			  International Conference on Learning Representations (ICLR)</a>  
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://icml.cc/">
			  International Conference on Machine Learning (ICML)</a>  
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://www.thecvf.com/?page_id=100">
			  IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</a>  
			  <font color="red"><strong>Outstanding Reviewer</strong></font>
			  <br/>
 			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://www.thecvf.com/?page_id=100">
			  IEEE International Conference on Computer Vision (ICCV)</a>  
			  <br/>
 			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://www.thecvf.com/?page_id=100">
			  European Conference on Computer Vision (ECCV)</a>  
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://www.aclweb.org/portal/acl">
			  Annual Meeting of the Association for Computational Linguistics (ACL)</a> 
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://sigdat.org/events">
			  Conference on Empirical Methods in Natural Language Processing (EMNLP)</a>  
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://eacl.org/">
			  Conference of the European Chapter of the Association for Computational Linguistics (EACL)</a> 
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://naacl.org/">
			  Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</a> 
			  <br/>			  
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://coling2020.org/">
			  International Conference on Computational Linguistics (COLING)</a>  
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="http://aaclweb.org/">
			  Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (AACL)</a>  
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://aclrollingreview.org/people">
			  Association for Computational Linguistics (ACL) Rolling Review (ARR)</a>  
			  <br/>
 			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://aaai.org/Conferences/AAAI/aaai.php">
			  AAAI Conference on Artificial Intelligence (AAAI)</a>  
			  <font color="red"><strong>Top Reviewer</strong></font>
 			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://www.ijcai.org/">
			  International Joint Conference on Artificial Intelligence (IJCAI)</a> 
			  <font color="red"><strong>Distinguished Reviewer</strong></font>
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="http://www.miccai.org/">
			  International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</a>  
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://wcci2022.org/">
			  IEEE World Congress on Computational Intelligence (IEEE WCCI)</a>  
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://www.ijcnn.org/">
			  International Joint Conference on Neural Networks (IJCNN)</a>  
			  <br/><br/>
		  <li> <strong>[Journal Reviewer]</strong>: <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">
			  IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI)</a> 
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">
			  IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT)</a> 
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="http://www.signalprocessingsociety.org/tmm/">
			  IEEE Transactions on Multimedia (IEEE TMM)</a> 
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://signalprocessingsociety.org/publications-resources/ieeeacm-transactions-audio-speech-and-language-processing">
			  IEEE/ACM Transactions on Audio Speech and Language Processing (IEEE/ACM TASLP)</a> 
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="https://www.journals.elsevier.com/engineering-applications-of-artificial-intelligence">
			  Engineering Applications of Artificial Intelligence (EAAI)</a> 
			  <br/>
			  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href="http://www.aas.net.cn/">
			  IEEE/CAA Journal of Automatica Sinica (自动化学报)</a>
			  <br/>
          </p>
        </td>
      </tr>
</table>

 
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>🎖 Honors & Awards</heading>
          <p>
		  <li> <strongsmall>[2022]</strongsmall> &nbsp;&nbsp;<smalll>
			  Clarendon Scholarship, University of Oxford, UK.</smalll><br/>
		  <li> <strongsmall>[2022]</strongsmall> &nbsp;&nbsp;<smalll>
			  Magdalen Graduate Scholarship, Magdalen College, UK.</smalll><br/>
		  <li> <strongsmall>[2022]</strongsmall> &nbsp;&nbsp;<smalll>
			  IJCAI-2022 Distinguished Reviewer Award (<font color="red"><strong>Top 3.0%</strong></font>).</smalll><br/>
		  <li> <strongsmall>[2022]</strongsmall> &nbsp;&nbsp;<smalll>
			  Outstanding Graduates of Beijing, China.</smalll><br/>
		  <li> <strongsmall>[2022]</strongsmall> &nbsp;&nbsp;<smalll>
			  Outstanding Graduates of Peking University, China.</smalll><br/>
		  <li> <strongsmall>[2022]</strongsmall> &nbsp;&nbsp;<smalll>
			  Scholarship of Academic Excellence (<font color="red"><strong>Top 1.0%</strong></font>), Peking University, China.</smalll><br/>
		  <li> <strongsmall>[2021]</strongsmall> &nbsp;&nbsp;<smalll>
			  Exceptional Award for Academic Innovation (<font color="red"><strong>Top 0.6%</strong></font>), Peking University, China.</smalll><br/>
		  <li> <strongsmall>[2021]</strongsmall> &nbsp;&nbsp;<smalll>
			  National Scholarship (<font color="red"><strong>Top 3.0%</strong></font>), China.</smalll><br/>
		  <li> <strongsmall>[2021]</strongsmall> &nbsp;&nbsp;<smalll>
			  Merit Student Pacesetter (<font color="red"><strong>Top 1.0%</strong></font>), Peking University, China.</smalll><br/>
		  <li> <strongsmall>[2021]</strongsmall> &nbsp;&nbsp;<smalll>
			  Scholarship of Academic Excellence (<font color="red"><strong>Top 1.0%</strong></font>), Peking University, China.</smalll><br/>
		  <li> <strongsmall>[2021]</strongsmall> &nbsp;&nbsp;<smalll>
			  CVPR-2021 Outstanding Reviewer Award.</smalll><br/>
		  <li> <strongsmall>[2021]</strongsmall> &nbsp;&nbsp;<smalll>
			  AAAI-2021 Top Reviewer Award.</smalll><br/>
		  <li> <strongsmall>[2020]</strongsmall> &nbsp;&nbsp;<smalll>
			  AAAI-2020 Scholarship.</smalll><br/>
		  <li> <strongsmall>[2020]</strongsmall> &nbsp;&nbsp;<smalll>
			  Exceptional Award for Academic Innovation (<font color="red"><strong>Top 0.6%</strong></font>), Peking University, China.</smalll><br/>
		  <li> <strongsmall>[2020]</strongsmall> &nbsp;&nbsp;<smalll>
			  National Scholarship (<font color="red"><strong>Top 3.0%</strong></font>), China.</smalll><br/>
		  <li> <strongsmall>[2020]</strongsmall> &nbsp;&nbsp;<smalll>
			  Merit Student Pacesetter (<font color="red"><strong>Top 1.0%</strong></font>), Peking University, China.</smalll><br/>
		  <li> <strongsmall>[2019]</strongsmall> &nbsp;&nbsp;<smalll>
			  NeurIPS-2019 Travel Award.</smalll><br/>
			  <!--
		  <li> <strongsmall>[2019]</strongsmall> &nbsp;&nbsp;<smalll>
			  Top-10 Students (<font color="red"><strong>Top 0.3%</strong></font>), School of ICE, Beijing University of Posts and Telecommunications, China.</smalll><br/>
		  <li> <strongsmall>[2019]</strongsmall> &nbsp;&nbsp;<smalll>
			  Outstanding Graduates of Beijing, China.</smalll><br/>
			  	-->

          </p>
        </td>
      </tr>
</table>
	  
<!--
		 
	      

<p></p> 
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <heading>&nbsp; &nbsp; &nbsp;Leadership Experience</heading>
      <td width="15%">
        <a href="image/ieee_activity1.jpg"><img src='image/ieee_activity1.jpg'  width="200" height="110"></a><br/><br/>
        <a href="image/ieee_activity2.jpg"><img src='image/ieee_activity2.jpg'  width="200" height="110"></a><br/><br/>
        &nbsp; <a href="image/ieee_activity3.jpg"><img src='image/ieee_activity3.jpg'  width="188" height="100"></a>
      </td>
      <td valign="top" width="75%">
        <stronghuge> <a>IEEE Student Branch</a></stronghuge> </br>
          <huge>[2019 - 2021]&nbsp;&nbsp; <em>President</em></huge><br />
          Organized academic forum, sharing sessions, Q&A meetings 7 times, serving over 100 graduate students on studying and future research planning. 
	The student branch grew from 30 to 120 people.<br/>        

        <p>
          <li> <strong><font color="red">Organized</font></strong> members to participate in NeurIPS 2020, COLING 2020 and ACM MM 2020.<br/>
          <li> <strong><font color="red">Helped</font></strong> members pubilsh several papers in conferences, e.g., AAAI, COLING, MICCAI, CIKM and ICASSP:<br/>
	<p> 
	
	7. Di You, <strong>Fenglin Liu</strong>, Shen Ge, Xiaoxia Xie, Jing Zhang, Xian Wu.<em> AlignTransformer: Hierarchical Alignment of Visual Regions and Disease Tags for Medical Report Generation</em>. 
        In Proceedings of <font color="#a82e2e"><strong>MICCAI 2021</font></strong>, <a>[paper link]</a><br/>
	6. Zhiqi Huang, <strong>Fenglin Liu</strong>, Xian Wu, Shen Ge, Helin Wang, Wei Fan.<em> Audio-Oriented Multimodal Machine Comprehension via Dynamic Inter- and Intra-modality Attention</em>.
        In Proceedings of <font color="#a82e2e"><strong>AAAI 2021</font></strong>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17548">[paper link]</a><br/>
	5. Bang Yang, Yuexian Zou, <strong>Fenglin Liu</strong>, Can Zhang.<em> Non-Autoregressive Coarse-to-Fine Video Captioning</em>. 
        In Proceedings of <font color="#a82e2e"><strong>AAAI 2021</font></strong>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/16421">[paper link]</a><br/>
	4. Zhiqi Huang, <strong>Fenglin Liu</strong>, Peilin Zhou.<em> Sentiment Injected Iteratively Co-Interactive Network for Spoken Language Understanding</em>. 
        In Proceedings of <font color="#a82e2e"><strong>ICASSP 2021</font></strong>, <a href="https://ieeexplore.ieee.org/document/9413885">[paper link]</a><br/>
	3. Nuo Chen, <strong>Fenglin Liu</strong>, Chenyu You, Peilin Zhou.<em> Adaptive Bi-Directional Attention: Exploring Multi-Granularity Representations for Machine Reading Comprehension</em>. 
        In Proceedings of <font color="#a82e2e"><strong>ICASSP 2021</font></strong>, <a href="https://ieeexplore.ieee.org/document/9414067">[paper link]</a><br/>
	2. Zhiqi Huang, <strong>Fenglin Liu</strong>, Yuexian Zou.<em> Federated Learning for Spoken Language Understanding</em>.  
        In Proceedings of <font color="#a82e2e"><strong>COLING 2020</font></strong>, <a href="https://www.aclweb.org/anthology/2020.coling-main.310/">[paper link]</a><br/>
	1. Yuanxin Liu, Zheng Lin, <strong>Fenglin Liu</strong>, Qinyun Dai, Weiping Wang.<em> Generating Paraphrase with Topic as Prior Knowledge</em>.  
        In Proceedings of <font color="#a82e2e"><strong>CIKM 2019</font></strong>, <a href="https://dl.acm.org/doi/10.1145/3357384.3358102">[paper link]</a>
          </p> 
      </td>
    </tr>
   </table>



<p></p> 
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <heading>&nbsp; &nbsp; &nbsp;Volunteerism</heading>
      <td width="15%"> 
        <a href="image/volunteer1.png"><img src='image/volunteer1.png' width="190" height="75"><br/><br/></a>
        <a href="image/volunteer2.png"><img src='image/volunteer2.png' width="190" height="75"><br/><br/></a> 
        <a href="image/volunteer3.png"><img src='image/volunteer3.png' width="190" height="75"></a>
      </td>
      <td valign="top" width="75%">
          <stronghuge><a href="https://www.bv2008.cn/cate/en/">Beijing Volunteer Service Federation</a> & Sunshine Volunteer Association</stronghuge></br>
          <huge>[2015 - 2019]&nbsp;&nbsp; <em>Volunteer</em></huge><br/></p>
          <li> During my college years, I took an active part in volunteer activities to help those in need, such as Elder Orphans, Children in Poor Areas, 
		  and Visitors from Other Cities or Foreign Countries, etc.
          <li> I have participated in 5 volunteer projects (including 21 volunteer activities), such as:<br/>
	<p> 

          2. <font color="red"><strong>Volunteering in Old People's Homes</strong></font>: 
		Along with other volunteers, I chatted with the elderly, performed shows for them, and taught them a series of fitness activities, 
		which improves their physical and mental health. We also helped them clean the room and do the laundry.<br/>

          1. <font color="red"><strong>Education Support Volunteer</strong></font>: 
		Due to the lack of teachers in the poor areas, the local teachers were overburdened with their duties, 
		which results in the local children not being able to enjoy high-quality education. Motivated by this problem, 
		I (along with other volunteers) went to a rural elementary school to relieve the burden of the teachers and helped the students build valuable skills. 
		We did some educational activities, including teaching the students how to improve reading skills and organizing outdoor discovery and learning activities, 
		endowing them with a broader range of konwledge.
      </td>
    </tr>
   </table>



<p></p> 
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <heading>&nbsp; &nbsp; &nbsp;Personal Interests</heading>
      <td width="15%">
        <a href="image/travel.png"><img src='image/travel.png'  width="190" height="100"></a><br/> <br/> 
        <a href="image/LOL.png"><img src='image/LOL.png'  width="190" height="110"></a>
      </td>
      <td valign="top" width="75%">
          <stronghuge><a>Tourism</a></stronghuge></br>
          <huge>[2015 - Present]&nbsp;&nbsp; <em>Tourist</em></huge><br/></p>
          <li> Whenever I had a long vacation (>3 days), I would travel to a new city for the pleasure releasing. <br/>
          <li> When traveling to a city, I will go to learn about the local culture, taste the local food, visit the local famous attractions/landmarks, and buy local souvenirs for my family and friends.
          <li> I have traveled to more than 40 cities. <br/><br/>

        <stronghuge><a href="https://na.leagueoflegends.com/en-us/">League of Legends (LOL)</a></stronghuge> </br>
          <huge>[2012 - 2019]&nbsp;&nbsp; <em>Player</em></huge><br />
       </p>
          <li> LOL, which is one of the best Multiplayer Online Battle Arena (MOBA) video games and the largest eSports in the world, accompanied me throughout my high school and college leisure time. <br/>
          <li> On March 15, 2017, I was promoted to Challenger Tier (<font color="red"><strong>Top 0.01%</strong></font>), which is the highest tier in LOL.
      </td>
    </tr>
   </table>


-->			

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="middle"><font size="1">
				<a title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=ZFc9QV2tDYMDP1RFxVtHNRh-DOmgZ3KsqolwcgR-1u4&cl=ffffff"></a>
				<br/> <br/>
				This awesome template was borrowed from  <a href="https://people.eecs.berkeley.edu/~barron/">this guy</a>~	
				</tbody></table>
<!---->
	
				
				
				
<!--
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="middle"><font size="2">
				This awesome template was borrowed from  <a href="https://people.eecs.berkeley.edu/~barron/">this guy</a>~
				</tbody></table>
				href="" 
-->
				
				
  </body>
</html>
